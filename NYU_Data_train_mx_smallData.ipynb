{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading NYU Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "import timeit\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_files=[]\n",
    "        folders=os.listdir(data_dir+'/RGB')\n",
    "        for folder in folders:\n",
    "            subfolders=os.listdir(data_dir+'/RGB/'+folder)\n",
    "            for subfolder in subfolders:\n",
    "                if subfolder.startswith('.'):\n",
    "                    continue\n",
    "                files=os.listdir(data_dir+'/RGB/'+folder+'/'+subfolder)\n",
    "                for file in files:\n",
    "                    if file.endswith('.mat'):\n",
    "                        self.data_files.append(folder+'/'+ subfolder+'/'+file)\n",
    "            self.data_dir=data_dir\n",
    "#         sort(self.data_files)      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name=self.data_files[index]\n",
    "        img=torch.from_numpy(h5py.File(self.data_dir+'/RGB/'+name,'r')['rgbOut'].value).float()\n",
    "        depth=torch.from_numpy(h5py.File(self.data_dir+'/DEP/'+name,'r')['depthOut'].value).float()\n",
    "        return img,depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset=DepthDataset('../Data_small')\n",
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((torch.randperm(self.num_samples)+self.start).long())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=dataset.__len__()\n",
    "NUM_TRAIN = int(N*0.9)\n",
    "NUM_VAL = N-NUM_TRAIN\n",
    "print(\"NUM_TRAIN:\",NUM_TRAIN,\",NUM_VAL:\",NUM_VAL)\n",
    "batch_size=6\n",
    "loader_train = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(NUM_TRAIN, 0),num_workers=8)\n",
    "loader_val = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN),num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image size = [304, 228]\n",
    "# depth size = [74,55]\n",
    "print(len(loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet50 = torch.nn.Sequential(*mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cropMore(nn.Module):\n",
    "    def forward(self,x):\n",
    "        N,C,H,W = x.size()\n",
    "        return x[:,:,0:H-6,0:W-9].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class unpooling(nn.Module):\n",
    "    def __init__(self,H,W):\n",
    "        super(unpooling, self).__init__()\n",
    "        self.indices = torch.zeros(H,W)\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                self.indices[i,j] = i*2*2*W+j*2\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "    def forward(self, x):\n",
    "        N,C,H,W = x.size()\n",
    "        indices = self.indices.expand(N,C,H,W)\n",
    "        y = self.unpool(x, Variable(indices.type(torch.cuda.LongTensor),requires_grad=False))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class upsampling(nn.Module):\n",
    "    def __init__(self,C,H,W):\n",
    "        super(upsampling, self).__init__()\n",
    "        self.unpooling = unpooling(H,W)\n",
    "        self.conv5 = nn.Conv2d(C,int(C/2),5,stride = 1,padding=2)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(int(C/2),int(C/2),3,stride = 1,padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(C,int(C/2),5,stride = 1,padding=2)\n",
    "    def forward(self, x):\n",
    "        y0 = self.unpooling(x)\n",
    "        y1 = self.conv5(y0)\n",
    "        y1 = self.ReLU(y1)\n",
    "        y1 = self.conv3(y1)\n",
    "        y2 = self.conv5_2(y0)\n",
    "        y3 = y1+y2\n",
    "        y = self.ReLU(y3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upsample = torch.nn.Sequential(\n",
    "    nn.Conv2d(2048,1024,1,stride = 1,padding=0),\n",
    "    nn.BatchNorm2d(1024), #1024*10*8\n",
    "    upsampling(1024,10,8),\n",
    "    upsampling(512,20,16),\n",
    "    upsampling(256,40,32),\n",
    "    #upsampling(128,80,64),\n",
    "    nn.Conv2d(128,1,3,stride = 1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    cropMore()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resnet50.type(dtype)\n",
    "upsample.type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t,(x,y) in enumerate(loader_train):\n",
    "    x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "    #pred=new_model(x_var)\n",
    "    x1 = resnet50(x_var)\n",
    "    pred = upsample(x1)\n",
    "    print(pred.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testModel = nn.MaxPool2d(2,2,return_indices = True)\n",
    "testModel.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_var=Variable(torch.zeros(1,3,6,10).type(dtype),requires_grad=False)\n",
    "#pred=new_model(x_var)\n",
    "y,ind = testModel(x_var)\n",
    "print(x_var.size())\n",
    "print(y.size())\n",
    "print(ind.size())\n",
    "print(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testModel = unpooling(10,8)\n",
    "testModel.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_var=Variable(torch.ones(1,4,10,8).type(dtype),requires_grad=False)\n",
    "#pred=new_model(x_var)\n",
    "y = testModel(x_var)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#images_var=Variable(images_pytorch.type(dtype),requires_grad=False)\n",
    "#depths_var=Variable(depths_pytorch.type(dtype),requires_grad=False)\n",
    "\n",
    "def loss_log(pred,y):\n",
    "    ep = 1e-6\n",
    "    N,W,H = pred.size()\n",
    "    pred = pred.contiguous().view(N,-1)\n",
    "    y = y.view(N,-1)\n",
    "    y = y+ep\n",
    "    d = pred - y.log()\n",
    "    d[y <= 0] = 0\n",
    "    n = W*H\n",
    "    loss = (d.pow(2).sum(1) / n - 0.5 / n/n * d.sum(1).pow(2)).sum()\n",
    "    loss /= N\n",
    "    return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=1e-6\n",
    "reg=1e-4\n",
    "adam_optim2=optim.Adam(resnet50.parameters(),lr=lr,weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr=1e-5\n",
    "reg=1e-4\n",
    "adam_optim=optim.Adam(upsample.parameters(),lr=lr,weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "print_every=50\n",
    "\n",
    "def train(model, loss_fn, optimizer, num_epochs = 1, plot_every = 2):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set the model to training mode, only effect batchnorm and dropout\n",
    "        avg_train_loss=0\n",
    "        num_batches=0\n",
    "        for t,(x,y) in enumerate(loader_train):\n",
    "            x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "            y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "            x1 = resnet50(x_var)\n",
    "            pred = model(x1)\n",
    "            loss = loss_fn(pred, y_var)\n",
    "            #losses.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            if (t+1) % print_every==0:\n",
    "                print('t = %d, loss = %.4f' % (t+1, loss.data[0]))\n",
    "            avg_train_loss+=loss.data[0]\n",
    "            num_batches+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del x,y\n",
    "        avg_train_loss/=num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        num_batches=0\n",
    "        avg_val_loss=0\n",
    "        for t,(x,y) in enumerate(loader_val):\n",
    "            x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "            y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "            x1 = resnet50(x_var)\n",
    "            pred = model(x1)\n",
    "            loss=loss_fn(pred,y_var)\n",
    "            avg_val_loss+=loss.data[0]\n",
    "            num_batches+=1\n",
    "            del x,y\n",
    "        avg_val_loss/=num_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(\"epoch:\",epoch,\"average training loss: %.2f\"%avg_train_loss,\"validation loss: %.2f\" %avg_val_loss)\n",
    "        if(epoch % plot_every == 0):\n",
    "            #with open('losses_ep'+ str(epoch)+ '.pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            #    pickle.dump([train_losses,val_losses], f)\n",
    "            #torch.save(model.state_dict(), 'alldata_dict_ep'+str(epoch))\n",
    "            plt.plot(train_losses)\n",
    "            plt.plot(val_losses)\n",
    "        \n",
    "            \n",
    "\n",
    "train(upsample,loss_log,adam_optim,num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(upsample,loss_log,adam_optim,num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "# visualize 5 images\n",
    "NUM_SHOW=6\n",
    "for t,(x,y) in enumerate(loader_train):\n",
    "    x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "    y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "    upsample.train(False)\n",
    "    x1 = resnet50(x_var)\n",
    "    pred = upsample(x1)\n",
    "    x_var=x_var.cpu()\n",
    "    y_var=y_var.cpu()\n",
    "    pred=pred.cpu()\n",
    "    for i in range(NUM_SHOW):\n",
    "        plt.subplot(3,NUM_SHOW,i+1)\n",
    "        img=x_var[i,:,:,:].data.numpy()\n",
    "        img=np.rollaxis(img,0,3)\n",
    "        imshow_noax(img,False)\n",
    "        plt.subplot(3,NUM_SHOW,NUM_SHOW+i+1)\n",
    "        img=y_var[i,:,:].data.numpy()\n",
    "        imshow_noax(img)\n",
    "        plt.subplot(3,NUM_SHOW,2*NUM_SHOW+i+1)\n",
    "        img=pred[i,:,:].data.numpy()\n",
    "        img = np.exp(img)\n",
    "        imshow_noax(img)\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(loader, model):\n",
    "    ep = 1e-7\n",
    "\n",
    "    thresh_1 = 0\n",
    "    thresh_2 = 0\n",
    "    abs_diff = 0\n",
    "    rmse = 0\n",
    "\n",
    "    for t,(x,y) in enumerate(loader):\n",
    "        x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "        y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "        model.train(False)\n",
    "        x1 = resnet50(x_var)\n",
    "        pred = model(x1)\n",
    "\n",
    "        #print(y_var.data.cpu().numpy())\n",
    "        y_var=y_var.data.cpu().numpy()\n",
    "        y_var = np.exp(y_var) + ep\n",
    "        pred=pred.data.cpu().numpy() + ep\n",
    "        num_var = np.shape(y_var)[0]*np.shape(y_var)[1]*np.shape(y_var)[2]\n",
    "\n",
    "        # threshold\n",
    "        thresh_mat = np.zeros_like(y_var)\n",
    "        thresh_mat[np.maximum(y_var / pred, pred / y_var) < 1.25] = 1\n",
    "        thresh_1 += np.sum(thresh_mat)\n",
    "        thresh_2 += num_var\n",
    "        #print('t1 = %d, t2 = %d', thresh_1, thresh_2)\n",
    "\n",
    "        # relative absolute diffe\n",
    "        abs_diff += np.sum(np.absolute(y_var - pred) / y_var) / num_var\n",
    "\n",
    "        rmse += np.sqrt(np.sum((y_var - pred) * (y_var - pred)) / num_var)\n",
    "        \n",
    "\n",
    "\n",
    "    abs_diff /= len(loader)\n",
    "    rmse /= len(loader)\n",
    "\n",
    "    print('percentage within threshold: ', thresh_1 / thresh_2)\n",
    "    print('relative absolute diff = ', abs_diff)\n",
    "    print('rmse = ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(loader_val, upsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, loss_fn, optimizer, optimizer2, num_epochs = 1, plot_every = 10):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set the model to training mode, only effect batchnorm and dropout\n",
    "        avg_train_loss=0\n",
    "        num_batches=0\n",
    "        for t,(x,y) in enumerate(loader_train):\n",
    "            x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "            y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "            x1 = resnet50(x_var)\n",
    "            pred = model(x1)\n",
    "            loss = loss_fn(pred, y_var)\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            if (t+1) % print_every==0:\n",
    "                print('t = %d, loss = %.4f' % (t+1, loss.data[0]))\n",
    "            avg_train_loss+=loss.data[0]\n",
    "            num_batches+=1\n",
    "            optimizer.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer2.step()\n",
    "            del x,y\n",
    "        avg_train_loss/=num_batches\n",
    "        num_batches=0\n",
    "        avg_val_loss=0\n",
    "        for t,(x,y) in enumerate(loader_val):\n",
    "            x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "            y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "            x1 = resnet50(x_var)\n",
    "            pred = model(x1)\n",
    "            loss=loss_fn(pred,y_var)\n",
    "            avg_val_loss+=loss.data[0]\n",
    "            num_batches+=1\n",
    "            del x,y\n",
    "        avg_val_loss/=num_batches\n",
    "        print(\"epoch:\",epoch,\"average training loss: %.2f\"%avg_train_loss,\"validation loss: %.2f\" %avg_val_loss)\n",
    "        if(epoch % plot_every == 0):\n",
    "            plt.plot(losses)\n",
    "            \n",
    "\n",
    "train(upsample,log_loss,adam_optim,adam_optim2,num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del upsample\n",
    "del resnet50\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "# visualize 5 images\n",
    "NUM_SHOW=5\n",
    "for t,(x,y) in enumerate(loader_train):\n",
    "    x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "    y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "    upsample.train(False)\n",
    "    x1 = resnet50(x_var)\n",
    "    pred = upsample(x1)\n",
    "    x_var=x_var.cpu()\n",
    "    y_var=y_var.cpu()\n",
    "    pred=pred.cpu()\n",
    "    for i in range(NUM_SHOW):\n",
    "        plt.subplot(3,NUM_SHOW,i+1)\n",
    "        img=x_var[i,:,:,:].data.numpy()\n",
    "        img=np.rollaxis(img,0,3)\n",
    "        imshow_noax(img,False)\n",
    "        plt.subplot(3,NUM_SHOW,NUM_SHOW+i+1)\n",
    "        img=y_var[i,:,:].data.numpy()\n",
    "        imshow_noax(img)\n",
    "        plt.subplot(3,NUM_SHOW,2*NUM_SHOW+i+1)\n",
    "        img=pred[i,:,:].data.numpy()\n",
    "        img = np.exp(img)\n",
    "        imshow_noax(img)\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(upsample,log_loss,adam_optim,adam_optim2,num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_SHOW=5\n",
    "for t,(x,y) in enumerate(loader_train):\n",
    "    x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "    y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "    upsample.train(False)\n",
    "    x1 = resnet50(x_var)\n",
    "    pred = upsample(x1)\n",
    "    x_var=x_var.cpu()\n",
    "    y_var=y_var.cpu()\n",
    "    pred=pred.cpu()\n",
    "    for i in range(NUM_SHOW):\n",
    "        plt.subplot(3,NUM_SHOW,i+1)\n",
    "        img=x_var[i,:,:,:].data.numpy()\n",
    "        img=np.rollaxis(img,0,3)\n",
    "        imshow_noax(img,False)\n",
    "        plt.subplot(3,NUM_SHOW,NUM_SHOW+i+1)\n",
    "        img=y_var[i,:,:].data.numpy()\n",
    "        imshow_noax(img)\n",
    "        plt.subplot(3,NUM_SHOW,2*NUM_SHOW+i+1)\n",
    "        img=pred[i,:,:].data.numpy()\n",
    "        img = np.exp(img)\n",
    "        imshow_noax(img)\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    ep = 1e-7\n",
    "\n",
    "    thresh_1 = 0\n",
    "    thresh_2 = 0\n",
    "    abs_diff = 0\n",
    "    rmse = 0\n",
    "\n",
    "    for t,(x,y) in enumerate(loader):\n",
    "        x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "        y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "        coarse_model.train(False)\n",
    "        pred=coarse_model(x_var)\n",
    "\n",
    "        #print(y_var.data.cpu().numpy())\n",
    "        y_var=y_var.data.cpu().numpy() + ep\n",
    "        pred=pred.data.cpu().numpy() + ep\n",
    "        num_var = np.shape(y_var)[0]*np.shape(y_var)[1]*np.shape(y_var)[2]\n",
    "\n",
    "        # threshold\n",
    "        thresh_mat = np.zeros_like(y_var)\n",
    "        thresh_mat[np.maximum(y_var / pred, pred / y_var) < 1.25] = 1\n",
    "        thresh_1 += np.sum(thresh_mat)\n",
    "        thresh_2 += num_var\n",
    "        #print('t1 = %d, t2 = %d', thresh_1, thresh_2)\n",
    "\n",
    "        # relative absolute diffe\n",
    "        abs_diff += np.sum(np.absolute(y_var - pred) / y_var) / num_var\n",
    "\n",
    "        rmse += np.sqrt(np.sum((y_var - pred) * (y_var - pred)) / num_var)\n",
    "\n",
    "\n",
    "    abs_diff /= len(loader_train)\n",
    "\n",
    "    print('percentage within threshold: ', thresh_1 / thresh_2)\n",
    "    print('relative absolute diff = ', abs_diff)\n",
    "    print('rmse = ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate(loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_overfit(model, loss_fn, optimizer, num_epochs = 1, plot_every = 10):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set the model to training mode, only effect batchnorm and dropout\n",
    "        avg_train_loss=0\n",
    "        num_batches=0\n",
    "        for t,(x,y) in enumerate(loader_train):\n",
    "            x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "            y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "            pred=model(x_var)\n",
    "            loss = loss_fn(pred, y_var)\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            if (t+1) % print_every==0:\n",
    "                print('t = %d, loss = %.4f' % (t+1, loss.data[0]))\n",
    "            avg_train_loss+=loss.data[0]\n",
    "            num_batches+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            break\n",
    "        avg_train_loss/=num_batches\n",
    "        num_batches=0\n",
    "\n",
    "        print(\"epoch:\",epoch,\"average training loss: %.2f\"%avg_train_loss)\n",
    "        if(epoch % plot_every == 0):\n",
    "            plt.plot(losses)\n",
    "            \n",
    "\n",
    "train_overfit(coarse_model,loss_log,adam_optim,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_SHOW=5\n",
    "for t,(x,y) in enumerate(loader_train):\n",
    "    x_var=Variable(x.type(dtype),requires_grad=False)\n",
    "    y_var=Variable(y.type(dtype),requires_grad=False)\n",
    "    coarse_model.train(False)\n",
    "    pred=coarse_model(x_var)\n",
    "    x_var=x_var.cpu()\n",
    "    y_var=y_var.cpu()\n",
    "    pred=pred.cpu()\n",
    "    for i in range(NUM_SHOW):\n",
    "        plt.subplot(3,NUM_SHOW,i+1)\n",
    "        img=x_var[i,:,:,:].data.numpy()\n",
    "        img=np.rollaxis(img,0,3)\n",
    "        imshow_noax(img,False)\n",
    "        plt.subplot(3,NUM_SHOW,NUM_SHOW+i+1)\n",
    "        img=y_var[i,:,:].data.numpy()\n",
    "        imshow_noax(img)\n",
    "        plt.subplot(3,NUM_SHOW,2*NUM_SHOW+i+1)\n",
    "        img=pred[i,:,:].data.numpy()\n",
    "        img = np.exp(img)\n",
    "        imshow_noax(img)\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "a = [1,2,3,4]#np.array([1,2,3,4])\n",
    "a.append(5)\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.zeros(3,3)\n",
    "a[0,1] = 2\n",
    "a[0,2] = 4\n",
    "b = torch.sum(a,0)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UnConv(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        channel1 = nn.Sequential(\n",
    "            nn.MaxUnpool2d(kernel_size=2, stride=None, padding=0),\n",
    "            nn.Conv2d(C,int(C/2),5,stride = 1,padding=2),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        channel1.type(dtype)\n",
    "        return channel1(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
